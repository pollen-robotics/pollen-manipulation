{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc7d95d-5d3b-4df3-8b37-214fccf579c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simsim/.virtualenvs/poc_llm_grasping/lib/python3.10/site-packages/pydantic/_internal/_config.py:334: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[06/18/24 17:16:37] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Your inference package version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span> is out of date! Please upgrade to <a href=\"file:///home/simsim/.virtualenvs/poc_llm_grasping/lib/python3.10/site-packages/inference/core/__init__.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/simsim/.virtualenvs/poc_llm_grasping/lib/python3.10/site-packages/inference/core/__init__.py#35\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.12</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of inference for the latest features and bug fixes by    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         running `pip install --upgrade inference`.                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[06/18/24 17:16:37]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Your inference package version \u001b[1;36m0.9\u001b[0m.\u001b[1;36m13\u001b[0m is out of date! Please upgrade to \u001b]8;id=580022;file:///home/simsim/.virtualenvs/poc_llm_grasping/lib/python3.10/site-packages/inference/core/__init__.py\u001b\\\u001b[2m__init__.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=824158;file:///home/simsim/.virtualenvs/poc_llm_grasping/lib/python3.10/site-packages/inference/core/__init__.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         version \u001b[1;36m0.12\u001b[0m.\u001b[1;36m1\u001b[0m of inference for the latest features and bug fixes by    \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         running `pip install --upgrade inference`.                              \u001b[2m              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[194430103135782700] [7.1] [1.800] [ColorCamera(1)] [warning] Unsupported resolution set for detected camera OV9782, needs 800_P or 720_P. Defaulting to 800_P\n",
      "[194430103135782700] [7.1] [1.800] [ColorCamera(0)] [warning] Unsupported resolution set for detected camera OV9782, needs 800_P or 720_P. Defaulting to 800_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cameras not initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model func:  <module 'contact_graspnet_pytorch.contact_graspnet' from '/home/simsim/Pollen/ai-dev/grasping/contact_graspnet_pytorch/contact_graspnet_pytorch/contact_graspnet_pytorch/contact_graspnet.py'>\n",
      "/home/simsim/.cache/huggingface/hub/models--pollen-robotics--contact_graspnet/snapshots/16f1311d6380e2e6d4394d250897585fea0258a2/checkpoints/contact_graspnet/checkpoints/model.pt\n",
      "=> Loading checkpoint from local file...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from pollen_manipulation.reachy_2.api import Reachy2ManipulationAPI\n",
    "\n",
    "from pollen_vision.camera_wrappers.depthai import SDKWrapper\n",
    "from pollen_vision.camera_wrappers.depthai.utils import get_config_file_path\n",
    "from pollen_vision.perception import Perception\n",
    "\n",
    "import FramesViewer.utils as fv_utils\n",
    "from reachy2_sdk import ReachySDK\n",
    "\n",
    "import numpy as np\n",
    "from pyquaternion import Quaternion\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "T_world_cam = fv_utils.make_pose([0.049597, 0.009989, 0.038089], [0, 0, 0])\n",
    "T_world_cam[:3, :3] = np.array([[0, 0, 1], [-1, 0, 0], [0, -1, 0]])\n",
    "T_world_cam = fv_utils.rotateInSelf(T_world_cam, [-48, 0, 0])\n",
    "\n",
    "cam = SDKWrapper(get_config_file_path(\"CONFIG_SR\"), compute_depth=True)\n",
    "\n",
    "K = cam.get_K()\n",
    "\n",
    "reachy = ReachySDK(host=\"localhost\")\n",
    "manipulation_api = Reachy2ManipulationAPI(reachy, T_world_cam, K)\n",
    "\n",
    "perception = Perception(camera_wrapper=cam, T_world_cam=T_world_cam, freq=10.0)\n",
    "perception.start(visualize=False)\n",
    "\n",
    "time.sleep(1.0)\n",
    "manipulation_api.turn_robot_on()\n",
    "time.sleep(1.0)\n",
    "manipulation_api.goto_rest_position(left=False)\n",
    "manipulation_api.goto_rest_position(left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a18a2c7c-95cc-48e6-86b7-65a96a09de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_info():\n",
    "    object_info = [\n",
    "        info for info in perception.get_objects_infos() if info[\"name\"] == object_name and info[\"detection_score\"] > 0.7\n",
    "    ]\n",
    "\n",
    "    if len(object_info) == 0:\n",
    "        print(f\"No {object_name} found\")\n",
    "        return []\n",
    "\n",
    "    elif len(object_info) == 1:\n",
    "        bottle_info = object_info[0]\n",
    "        print(f\"Found a {object_name}\")\n",
    "        return object_info[0]\n",
    "    else:\n",
    "        print(f\"Found {len(object_info)} {object_name}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def compute_goal_diff(fk_matrix, goal_pose):\n",
    "    return np.linalg.norm(fk_matrix - selected_array)\n",
    "\n",
    "\n",
    "def compute_l2_distance(fk_matrix, goal_pose):\n",
    "    return np.linalg.norm(fk_matrix[:-1, 3] - goal_pose[:-1, 3])\n",
    "\n",
    "\n",
    "def compute_rotation_distance(fk_matrix, goal_pose):\n",
    "    q1 = Quaternion(matrix=fk_matrix[:3, :3])\n",
    "    Q, R = np.linalg.qr(goal_pose[:3, :3])\n",
    "    q2 = Quaternion(matrix=Q)\n",
    "    return Quaternion.distance(q1, q2)\n",
    "\n",
    "\n",
    "def produce_symetrical_poses(grasp_poses):\n",
    "    symetrical_grasp_poses = []\n",
    "\n",
    "    for grasp_pose in grasp_poses:\n",
    "        symetrical_grasp_pose = np.eye(4)\n",
    "        x, y, z = grasp_pose[:-1, 3]\n",
    "        symetrical_grasp_pose[:-1, 3] = [x, -y, z]\n",
    "\n",
    "        rotation_matrix = grasp_pose[:-1, :-1]\n",
    "        roll, pitch, yaw = R.from_matrix(rotation_matrix).as_euler(\"xyz\", degrees=True)\n",
    "        symetrical_grasp_pose[:-1, :-1] = R.from_euler(\"xyz\", [-roll, pitch, -yaw], degrees=True).as_matrix()\n",
    "        symetrical_grasp_poses.append(symetrical_grasp_pose)\n",
    "\n",
    "    return symetrical_grasp_poses\n",
    "\n",
    "\n",
    "# Define the initial pose matrices for each arm used by goto_rest_position API method\n",
    "right_start_pose = np.array(\n",
    "    [\n",
    "        [0.0, 0.0, -1.0, 0.20],\n",
    "        [0.0, 1.0, 0.0, -0.24],\n",
    "        [1.0, 0.0, 0.0, -0.23],\n",
    "        [0.0, 0.0, 0.0, 1.0],\n",
    "    ]\n",
    ")\n",
    "left_start_pose = np.array(\n",
    "    [\n",
    "        [0.0, 0.0, -1.0, 0.20],\n",
    "        [0.0, 1.0, 0.0, 0.24],\n",
    "        [1.0, 0.0, 0.0, -0.23],\n",
    "        [0.0, 0.0, 0.0, 1.0],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1074af4-14b7-436c-ad1f-12512d63a8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding tracking for object: mug\n"
     ]
    }
   ],
   "source": [
    "object_name = \"mug\"\n",
    "\n",
    "perception.set_tracked_objects([object_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8d1c9d-2a77-44e1-9a57-c231edbd4c95",
   "metadata": {},
   "source": [
    "## Trigger grasping on fake robot\n",
    "Takes the first reachable grasp pose in the list generated by graspnet and perform grasping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f50cf3-b304-442f-b036-1b76886aaf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_info = get_object_info()\n",
    "\n",
    "if object_info == []:\n",
    "    print(\"Can't try grasping\")\n",
    "else:\n",
    "    manipulation_api.grasp_object(object_info, left=True)\n",
    "    manipulation_api.goto_rest_position(left=True, open_gripper=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505d1082-3b7e-4b23-b21c-74cf482766e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_pose_test = np.array(\n",
    "    [\n",
    "        [-0.20852, 0.90069, -0.38116, 0.25596],\n",
    "        [-0.97297, -0.2306, -0.012651, 0.032521],\n",
    "        [-0.099292, 0.36822, 0.92442, -0.27823],\n",
    "        [0, 0, 0, 1],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bea097-9007-4848-ae33-d4e114fa9d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "manipulation_api.reachy.l_arm.goto_from_matrix(\n",
    "    grasp_pose_test,\n",
    "    duration=4.0,\n",
    "    with_cartesian_interpolation=True,\n",
    "    interpolation_frequency=120,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d7c577-2773-40d3-828a-a3e16b890752",
   "metadata": {},
   "outputs": [],
   "source": [
    "manipulation_api.goto_rest_position(left=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30506ae2-595b-41e8-b65e-18d4e54327bb",
   "metadata": {},
   "source": [
    "## Get all reachable grasp poses candidates\n",
    "\n",
    "First, get the list of all the reachable grasp poses. The rest position is also inserted as first position so that it is possible to put the arm back to the rest position with the slider when viewing the grasp pose candidates.\n",
    "\n",
    "A list with symetrical grasp poses is also generated. The symmetry of a given grasp pose for an arm is such that the other arm should end up in the same position as if it was with a mirror. Having this was useful to check the symmetry of both arms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bf7a6ba-d1b3-43aa-9391-602101efb2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a mug\n",
      "Contact graspnet infer\n",
      "Converting depth to point cloud(s)...\n",
      "using local regions\n",
      "Extracted Region Cube Size:  0.2055569589138031\n",
      "Generated 42 grasps for object 1\n",
      "scores: {1: array([    0.20185,     0.17081,     0.24072,     0.19778,     0.19382,      0.2287,     0.18245,     0.17023,     0.22833,     0.16916,     0.18102,     0.23397,     0.15406,     0.17552,     0.17659,     0.24251,     0.19576,     0.17041,     0.15877,     0.15319,     0.17558,     0.15148,     0.19326,     0.17902,\n",
      "           0.21155,     0.16044,     0.22172,     0.21657,     0.18653,     0.22172,     0.15968,      0.1896,     0.15781,      0.1801,     0.22678,     0.17673,     0.25597,     0.15821,     0.19237,     0.18193,     0.18866,     0.17456], dtype=float32)}\n",
      "SCORES: 1 (42,) (42, 4, 4) (42, 3)\n",
      "Number of grasp poses generated: 84\n",
      "Number of reachable grasp poses: 32\n"
     ]
    }
   ],
   "source": [
    "object_info = get_object_info()\n",
    "\n",
    "if object_info == []:\n",
    "    print(\"Can't get reachable grasp poses\")\n",
    "else:\n",
    "    pose = object_info[\"pose\"]\n",
    "    rgb = object_info[\"rgb\"]\n",
    "    mask = object_info[\"mask\"]\n",
    "    depth = object_info[\"depth\"]\n",
    "\n",
    "    # Put this flag to False if you want to grasp with the right arm\n",
    "    left = True\n",
    "\n",
    "    if left:\n",
    "        arm = manipulation_api.reachy.l_arm\n",
    "        symetrical_arm = manipulation_api.reachy.r_arm\n",
    "        start_pose = left_start_pose\n",
    "\n",
    "    else:\n",
    "        arm = manipulation_api.reachy.r_arm\n",
    "        symetrical_arm = manipulation_api.reachy.l_arm\n",
    "        start_pose = right_start_pose\n",
    "\n",
    "    grasp_poses, scores, _, _ = manipulation_api.get_reachable_grasp_poses(rgb, depth, mask, left)\n",
    "    grasp_poses.insert(0, start_pose)\n",
    "    scores.insert(0, 1.0)\n",
    "\n",
    "    symetrical_grasp_poses = produce_symetrical_poses(grasp_poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73dfd2c3-efe0-461a-b434-ff0f049e9aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "reachy.l_arm.publish_grasp_poses(grasp_poses[1:], scores[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "953961e4-6cd8-4b65-9a60-e373cb50adbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.5934533473305537,\n",
       " 1.4254119201925415,\n",
       " 1.0765629671195822,\n",
       " 0.9462017504997993,\n",
       " 0.8894371203159755,\n",
       " 0.870752036821702,\n",
       " 0.8167142853478704,\n",
       " 0.7802574463953191,\n",
       " 0.7761480863754101,\n",
       " 0.7736318900865764,\n",
       " 0.7726521119269948,\n",
       " 0.7503110248599777,\n",
       " 0.7319350004701093,\n",
       " 0.6744333973744249,\n",
       " 0.6622294448166383,\n",
       " 0.6397551604474674,\n",
       " -0.3341352822670091,\n",
       " -0.34601629904927556,\n",
       " -0.37786690872520023,\n",
       " -0.40039244942203633,\n",
       " -0.4038492999146086,\n",
       " -0.4077077080123586,\n",
       " -0.4101055312067974,\n",
       " -0.4164474011772739,\n",
       " -0.433277166314565,\n",
       " -0.43329866047410737,\n",
       " -0.4376630739123305,\n",
       " -0.4392427883561481,\n",
       " -0.44892821905262786,\n",
       " -0.45860696064272694,\n",
       " -0.46661777768476365,\n",
       " -0.46999770275510366]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce879d-7969-400b-91a0-1d9d4c5e4432",
   "metadata": {},
   "source": [
    "## Iterate over each reachable grasp pose candidate\n",
    "\n",
    "Using the slider, iterate over each reachable grasp pose and send it to the fake robot to visualize what would be the movement of the arm in rviz.\n",
    "\n",
    "Once the grasp pose reached, the l2 distance between the actual cartesian coordinates of the effector and the one from the grasp pose is computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c285848-8e6a-4bb0-8625-d4584d8146cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "slider = widgets.IntSlider(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=len(grasp_poses) - 1,\n",
    "    step=1,\n",
    "    description=\"Tableau:\",\n",
    "    orientation=\"horizontal\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"700px\"),  # Ajustez la largeur ici\n",
    ")\n",
    "\n",
    "# Bouton pour exécuter l'action\n",
    "button = widgets.Button(description=\"Send goal pose\")\n",
    "\n",
    "# Zone de sortie pour afficher les résultats\n",
    "output = widgets.Output()\n",
    "\n",
    "\n",
    "def on_slider_change(change):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        selected_index = change[\"new\"]\n",
    "        selected_array = grasp_poses[selected_index]\n",
    "        print(f\"Tableau {selected_index}: \")\n",
    "        print(selected_array)\n",
    "        print(f\"Pose score: {scores[selected_index]}\")\n",
    "\n",
    "        goto_id = arm.goto_from_matrix(\n",
    "            selected_array, with_cartesian_interpolation=True, interpolation_frequency=120, duration=2.0\n",
    "        )\n",
    "\n",
    "        # if goto_id != -1:\n",
    "        #     while not manipulation_api.reachy.is_move_finished(goto_id):\n",
    "        #         time.sleep(0.1)\n",
    "\n",
    "        #     time.sleep(1.0)\n",
    "\n",
    "        joints_pos = arm.get_joints_positions()\n",
    "        fk = arm.forward_kinematics()\n",
    "\n",
    "        l2_dist = compute_l2_distance(fk, selected_array)\n",
    "        print(f\"l2 dist: {l2_dist}\")\n",
    "\n",
    "        ## Uncomment the lines below to send symmetric grasp poses to the other arm\n",
    "\n",
    "        # goto_id = symetrical_arm.goto_from_matrix(symetrical_grasp_poses[selected_index])\n",
    "        # if goto_id != -1:\n",
    "        #     while not manipulation_api.reachy.is_move_finished(goto_id):\n",
    "        #         time.sleep(0.1)\n",
    "\n",
    "        #     time.sleep(1.0)\n",
    "        #     sym_fk = symetrical_arm.forward_kinematics()\n",
    "\n",
    "        #     l2_dist = compute_l2_distance(sym_fk, symetrical_grasp_poses[selected_index])\n",
    "        #     print(f'l2 dist for symetrical arm: {l2_dist}')\n",
    "        #     print(symetrical_grasp_poses[selected_index])\n",
    "\n",
    "\n",
    "# Associer la fonction de changement de valeur au slider\n",
    "slider.observe(on_slider_change, names=\"value\")\n",
    "\n",
    "# Afficher les widgets\n",
    "display(slider, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41b683f-1223-434f-9652-95d27c87e147",
   "metadata": {},
   "outputs": [],
   "source": [
    "arm.goto_from_matrix(left_start_pose, with_cartesian_interpolation=True, interpolation_frequency=120, duration=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c9a4a-de88-4199-9435-e2399db1e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "manipulation_api.goto_rest_position(left=True)\n",
    "manipulation_api.goto_rest_position(left=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85c8e1f-7729-4f08-a9ed-00d5cf2d4d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
