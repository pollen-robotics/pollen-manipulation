{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc7d95d-5d3b-4df3-8b37-214fccf579c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/22/24 18:24:44] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Your inference package version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span> is out of date! Please upgrade to <a href=\"file:///home/simsim/.virtualenvs/poc_llm_grasping/lib/python3.10/site-packages/inference/core/__init__.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/simsim/.virtualenvs/poc_llm_grasping/lib/python3.10/site-packages/inference/core/__init__.py#35\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.11</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> of inference for the latest features and bug fixes by    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         running `pip install --upgrade inference`.                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/22/24 18:24:44]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Your inference package version \u001b[1;36m0.9\u001b[0m.\u001b[1;36m13\u001b[0m is out of date! Please upgrade to \u001b]8;id=618361;file:///home/simsim/.virtualenvs/poc_llm_grasping/lib/python3.10/site-packages/inference/core/__init__.py\u001b\\\u001b[2m__init__.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=277977;file:///home/simsim/.virtualenvs/poc_llm_grasping/lib/python3.10/site-packages/inference/core/__init__.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         version \u001b[1;36m0.11\u001b[0m.\u001b[1;36m0\u001b[0m of inference for the latest features and bug fixes by    \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         running `pip install --upgrade inference`.                              \u001b[2m              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19443010C1D4782700] [3.1] [1.435] [ColorCamera(1)] [warning] Unsupported resolution set for detected camera OV9782, needs 800_P or 720_P. Defaulting to 800_P\n",
      "[19443010C1D4782700] [3.1] [1.436] [ColorCamera(0)] [warning] Unsupported resolution set for detected camera OV9782, needs 800_P or 720_P. Defaulting to 800_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cameras not initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model func:  <module 'contact_graspnet_pytorch.contact_graspnet' from '/home/simsim/.virtualenvs/poc_llm_grasping/lib/python3.10/site-packages/contact_graspnet_pytorch/contact_graspnet.py'>\n",
      "/home/simsim/.cache/huggingface/hub/models--pollen-robotics--contact_graspnet/snapshots/16f1311d6380e2e6d4394d250897585fea0258a2/checkpoints/contact_graspnet/checkpoints/model.pt\n",
      "=> Loading checkpoint from local file...\n",
      "Adding tracking for object: bottle\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pollen_manipulation.reachy_2.api import Reachy2ManipulationAPI\n",
    "\n",
    "from pollen_vision.camera_wrappers.depthai import SDKWrapper\n",
    "from pollen_vision.camera_wrappers.depthai.utils import get_config_file_path\n",
    "from pollen_vision.perception import Perception\n",
    "\n",
    "import FramesViewer.utils as fv_utils\n",
    "from reachy2_sdk import ReachySDK\n",
    "\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "T_world_cam = fv_utils.make_pose([0.052644, 0.01, 0.034150], [0, 0, 0])\n",
    "T_world_cam[:3, :3] = np.array([[0, 0, 1], [-1, 0, 0], [0, -1, 0]])\n",
    "T_world_cam = fv_utils.rotateInSelf(T_world_cam, [-48, 0, 0])\n",
    "\n",
    "cam = SDKWrapper(get_config_file_path(\"CONFIG_SR\"), compute_depth=True)\n",
    "\n",
    "K = cam.get_K()\n",
    "\n",
    "reachy = ReachySDK(host='localhost')\n",
    "manipulation_api = Reachy2ManipulationAPI(reachy, T_world_cam, K)\n",
    "\n",
    "perception = Perception(\n",
    "    camera_wrapper=cam, T_world_cam=T_world_cam, freq=10.0\n",
    ")\n",
    "perception.start(visualize=False)\n",
    "\n",
    "perception.set_tracked_objects(['bottle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b850cfc-d8a5-49bf-b3bb-9a6d11bff240",
   "metadata": {},
   "outputs": [],
   "source": [
    "manipulation_api.turn_robot_on()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bf51afe-5e38-40dc-806e-332e354a6838",
   "metadata": {},
   "outputs": [],
   "source": [
    "manipulation_api.goto_rest_position(left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a18a2c7c-95cc-48e6-86b7-65a96a09de73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a bottle\n"
     ]
    }
   ],
   "source": [
    "object_info = [info for info in perception.get_objects_infos() if info['name'] == 'bottle']\n",
    "\n",
    "if len(object_info) == 0:\n",
    "    print('No bottle found')\n",
    "\n",
    "elif len(object_info) == 1:\n",
    "    bottle_info = object_info[0]\n",
    "    print('Found a bottle')\n",
    "\n",
    "else:\n",
    "    print(f'Found {len(object_info)} bottles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b27d105-f1e3-4acc-8465-c4f366eee3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def compute_goal_diff(fk_matrix, goal_pose):\n",
    "    return np.linalg.norm(fk_matrix - selected_array)\n",
    "\n",
    "def compute_l2_distance(fk_matrix, goal_pose):\n",
    "    return np.linalg.norm(fk_matrix[:-1, 3] - goal_pose[:-1, 3])\n",
    "\n",
    "\n",
    "def produce_symetrical_poses(grasp_poses):\n",
    "    symetrical_grasp_poses = []\n",
    "\n",
    "    for grasp_pose in grasp_poses:\n",
    "        symetrical_grasp_pose = np.eye(4)\n",
    "        x, y, z = grasp_pose[:-1, 3]\n",
    "        symetrical_grasp_pose[:-1, 3] = [x, -y, z]\n",
    "\n",
    "        rotation_matrix = grasp_pose[:-1, :-1]\n",
    "        roll, pitch, yaw = R.from_matrix(rotation_matrix).as_euler(\"xyz\", degrees=True)\n",
    "        symetrical_grasp_pose[:-1, :-1] = R.from_euler(\"xyz\", [-roll, pitch, -yaw], degrees=True).as_matrix()\n",
    "        symetrical_grasp_poses.append(symetrical_grasp_pose)\n",
    "\n",
    "    return symetrical_grasp_poses\n",
    "\n",
    "# Define the initial pose matrices for each arm used by goto_rest_position API method\n",
    "right_start_pose = np.array([\n",
    "    [0.0, 0.0, -1.0, 0.20],\n",
    "    [0.0, 1.0, 0.0, -0.24],\n",
    "    [1.0, 0.0, 0.0, -0.23],\n",
    "    [0.0, 0.0, 0.0, 1.0],\n",
    "])\n",
    "left_start_pose = np.array([\n",
    "    [0.0, 0.0, -1.0, 0.20],\n",
    "    [0.0, 1.0, 0.0, 0.24],\n",
    "    [1.0, 0.0, 0.0, -0.23],\n",
    "    [0.0, 0.0, 0.0, 1.0],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bf7a6ba-d1b3-43aa-9391-602101efb2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting depth to point cloud(s)...\n",
      "using local regions\n",
      "Extracted Region Cube Size:  0.5699999332427979\n",
      "Generated 98 grasps for object 1\n",
      "Number of grasp poses: 196\n",
      "Number of reachable grasp poses: 16\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "pose = bottle_info[\"pose\"]\n",
    "rgb = bottle_info[\"rgb\"]\n",
    "mask = bottle_info[\"mask\"]\n",
    "depth = bottle_info[\"depth\"]\n",
    "\n",
    "left = False\n",
    "\n",
    "if left:\n",
    "    arm = manipulation_api.reachy.l_arm\n",
    "    symetrical_arm = manipulation_api.reachy.r_arm\n",
    "    start_pose = left_start_pose\n",
    "    \n",
    "else:\n",
    "    arm = manipulation_api.reachy.r_arm\n",
    "    symetrical_arm = manipulation_api.reachy.l_arm\n",
    "    start_pose = right_start_pose\n",
    "\n",
    "grasp_poses, _ = manipulation_api.get_reachable_grasp_poses(rgb, depth, mask, left)\n",
    "grasp_poses.insert(0, start_pose)\n",
    "\n",
    "symetrical_grasp_poses = produce_symetrical_poses(grasp_poses)\n",
    "\n",
    "print(len(grasp_poses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c285848-8e6a-4bb0-8625-d4584d8146cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e599a8ae850948989bb8b42087ec21b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0, description='Tableau:', max=3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ac551a4bf643c0abfc3b4965890f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "slider = widgets.IntSlider(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=len(grasp_pose) - 1,\n",
    "    step=1,\n",
    "    description='Tableau:',\n",
    "    orientation='horizontal',\n",
    ")\n",
    "\n",
    "# Bouton pour exécuter l'action\n",
    "button = widgets.Button(description=\"Send goal pose\")\n",
    "\n",
    "# Zone de sortie pour afficher les résultats\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_slider_change(change):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        selected_index = change['new']\n",
    "        selected_array = grasp_poses[selected_index]\n",
    "        print(f\"Tableau {selected_index + 1}:\")\n",
    "        print(selected_array)\n",
    "        \n",
    "        goto_id = arm.goto_from_matrix(selected_array)\n",
    "\n",
    "        if goto_id != -1:\n",
    "            while not manipulation_api.reachy.is_move_finished(goto_id):\n",
    "                time.sleep(0.1)\n",
    "\n",
    "            time.sleep(1.0)\n",
    "\n",
    "            joints_pos = arm.get_joints_positions()\n",
    "            fk = arm.forward_kinematics()\n",
    "        \n",
    "            l2_dist = compute_l2_distance(fk, selected_array)\n",
    "            print(f'l2 dist: {l2_dist}')\n",
    "\n",
    "        goto_id = symetrical_arm.goto_from_matrix(symetrical_grasp_poses[selected_index])\n",
    "        if goto_id != -1:\n",
    "            while not manipulation_api.reachy.is_move_finished(goto_id):\n",
    "                time.sleep(0.1)\n",
    "\n",
    "            time.sleep(1.0)\n",
    "                                                  \n",
    "# Associer la fonction de changement de valeur au slider\n",
    "slider.observe(on_slider_change, names='value')\n",
    "\n",
    "# Afficher les widgets\n",
    "display(slider, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c09c9a4a-de88-4199-9435-e2399db1e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "manipulation_api.goto_rest_position(left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8104acdd-0a7f-49e3-9719-6cfbfd8d4327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GoToHomeId(head=id: 79\n",
       ", r_arm=id: 80\n",
       ", l_arm=id: 81\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manipulation_api.reachy.set_pose('zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9095e92-0876-4850-9886-1ad796538311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7bee3d-2765-4db4-b45d-719cfd9ccdc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c5ce77-0be0-4991-a28a-63ac95683fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
